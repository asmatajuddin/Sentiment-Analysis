{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Latent Semantic Analysis"
      ],
      "metadata": {
        "id": "tl_GswJYYk6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discovering topics are beneficial for various purposes such as for clustering documents, organizing online available content for information retrieval and recommendations. Multiple content providers and news agencies are using topic models for recommending articles to readers. Similarly recruiting firms are using in extracting job descriptions and mapping them with candidate skill set. If you see the data scientist job, which is all about extracting the ‘knowledge’ from a large amount of collected data. Usually, collected data is unstructured. You need powerful tools and techniques to analyze and understand a large amount of unstructured data.\n",
        "\n",
        "Topic modeling is a text mining technique which provides methods for identifying co-occurring keywords to summarize large collections of textual information. It helps in discovering hidden topics in the document, annotate the documents with these topics, and organize a large amount of unstructured data.\n",
        "\n",
        "Topic Modeling\n",
        "\n",
        "Topic Modeling automatically discover the hidden themes from given documents. It is an unsupervised text analytics algorithm that is used for finding the group of words from the given document. These group of words represents a topic. There is a possibility that, a single document can associate with multiple themes. for example, a group words such as 'patient', 'doctor', 'disease', 'cancer', ad 'health' will represents topic 'healthcare'. Topic Modeling is a different game compared to rule-based text searching that uses regular expressions.\n",
        "\n",
        "diagram\n",
        "Comparison Between Text Classification and Topic Modeling\n",
        "\n",
        "Text classification is a supervised machine learning problem, where a text document or article classified into a pre-defined set of classes. Topic modeling is the process of discovering groups of co-occurring words in text documents. These group co-occurring related words makes \"topics\". It is a form of unsupervised learning, so the set of possible topics are unknown. Topic modeling can be used to solve the text classification problem. Topic modeling will identify the topics presents in a document\" while text classification classifies the text into a single class.\n",
        "\n",
        "diagram\n",
        "Latent Semantic Analysis\n",
        "\n",
        "LSA (Latent Semantic Analysis) also known as LSI (Latent Semantic Index) LSA uses bag of word(BoW) model, which results in a term-document matrix(occurrence of terms in a document). Rows represent terms and columns represent documents. LSA learns latent topics by performing a matrix decomposition on the document-term matrix using Singular value decomposition. LSA is typically used as a dimension reduction or noise reducing technique.\n",
        "\n",
        "Latent Semantic Analysis\n",
        "Singular Value Decomposition(SVD)\n",
        "\n",
        "SVD is a matrix factorization method that represents a matrix in the product of two matrices. It offers various useful applications in signal processing, psychology, sociology, climate, and atmospheric science, statistics and astronomy.\n",
        "\n",
        "factor\n",
        "M is an m×m matrix\n",
        "U is a m×n left singular matrix\n",
        "Σ is a n×n diagonal matrix with non-negative real numbers.\n",
        "V is a m×n right singular matrix\n",
        "V* is n×m matrix, which is the transpose of the V.\n",
        "Identity matrix: It is a square matrix in which all the elements of the principal diagonal are ones, and all other elements are zeros.\n",
        "\n",
        "Diagonal Matrix: It is a matrix in which the entries other than the main diagonal are all zero.\n",
        "\n",
        "Singular Matrix: A matrix is singular if its determinant is 0 or a square matrix that does not have a matrix inverse.\n",
        "\n",
        "Determining Optimum Number of Topics\n",
        "\n",
        "What is the best way to determine k (number of topics) in topic modeling? Identify the optimum number of topics in the given corpus text is a challenging task. We can use the following options for determining the optimum number of topics:\n",
        "\n",
        "One way to determine the optimum number of topics is to consider each topic as a cluster and find out the effectiveness of a cluster using the Silhouette coefficient.\n",
        "Topic coherence measure is a realistic measure for identifying the number of topics.\n",
        "Topic Coherence measure is a widely used metric to evaluate topic models. It uses the latent variable models. Each generated topic has a list of words. In topic coherence measure, you will find average/median of pairwise word similarity scores of the words in a topic. The high value of topic coherence score model will be considered as a good topic model.\n",
        "\n",
        "Implementing LSA using Gensim\n",
        "\n",
        "Import the required library"
      ],
      "metadata": {
        "id": "vXlLI_auYmU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import modules\n",
        "import os.path\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "ojSQWhz5YrHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path,file_name):\n",
        "    \"\"\"\n",
        "    Input  : path and file_name\n",
        "    Purpose: loading text file\n",
        "    Output : list of paragraphs/documents and\n",
        "             title(initial 100 words considred as title of document)\n",
        "    \"\"\"\n",
        "    documents_list = []\n",
        "    titles=[]\n",
        "    with open( os.path.join(path, file_name) ,\"r\") as fin:\n",
        "        for line in fin.readlines():\n",
        "            text = line.strip()\n",
        "            documents_list.append(text)\n",
        "    print(\"Total Number of Documents:\",len(documents_list))\n",
        "    titles.append( text[0:min(len(text),100)] )\n",
        "    return documents_list,titles\n"
      ],
      "metadata": {
        "id": "dnN9CfgnYsRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(doc_set):\n",
        "    \"\"\"\n",
        "    Input  : docuemnt list\n",
        "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
        "    Output : preprocessed text\n",
        "    \"\"\"\n",
        "    # initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # create English stop words list\n",
        "    en_stop = set(stopwords.words('english'))\n",
        "    # Create p_stemmer of class PorterStemmer\n",
        "    p_stemmer = PorterStemmer()\n",
        "    # list for tokenized documents in loop\n",
        "    texts = []\n",
        "    # loop through document list\n",
        "    for i in doc_set:\n",
        "        # clean and tokenize document string\n",
        "        raw = i.lower()\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        # remove stop words from tokens\n",
        "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "        # stem tokens\n",
        "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "        # add tokens to list\n",
        "        texts.append(stemmed_tokens)\n",
        "    return texts\n"
      ],
      "metadata": {
        "id": "w0CWn87gYuEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_corpus(doc_clean):\n",
        "    \"\"\"\n",
        "    Input  : clean document\n",
        "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
        "    Output : term dictionary and Document Term Matrix\n",
        "    \"\"\"\n",
        "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "    # generate LDA model\n",
        "    return dictionary,doc_term_matrix\n"
      ],
      "metadata": {
        "id": "XSKtng0mYvz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
        "    \"\"\"\n",
        "    Input  : clean document, number of topics and number of words associated with each topic\n",
        "    Purpose: create LSA model using gensim\n",
        "    Output : return LSA model\n",
        "    \"\"\"\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    # generate LSA model\n",
        "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
        "    return lsamodel\n"
      ],
      "metadata": {
        "id": "xx6TQuGSYxUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Input   : dictionary : Gensim dictionary\n",
        "              corpus : Gensim corpus\n",
        "              texts : List of input texts\n",
        "              stop : Max num of topics\n",
        "    purpose : Compute c_v coherence for various number of topics\n",
        "    Output  : model_list : List of LSA topic models\n",
        "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        # generate LSA model\n",
        "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values"
      ],
      "metadata": {
        "id": "HLeL9gsmYy3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Input   : dictionary : Gensim dictionary\n",
        "              corpus : Gensim corpus\n",
        "              texts : List of input texts\n",
        "              stop : Max num of topics\n",
        "    purpose : Compute c_v coherence for various number of topics\n",
        "    Output  : model_list : List of LSA topic models\n",
        "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        # generate LSA model\n",
        "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values"
      ],
      "metadata": {
        "id": "_Plsm5QTY0jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(doc_clean,start, stop, step):\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
        "                                                            stop, start, step)\n",
        "    # Show graph\n",
        "    x = range(start, stop, step)\n",
        "    plt.plot(x, coherence_values)\n",
        "    plt.xlabel(\"Number of Topics\")\n",
        "    plt.ylabel(\"Coherence score\")\n",
        "    plt.legend((\"coherence_values\"), loc='best')\n",
        "    plt.show()\n",
        "\n",
        "start,stop,step=2,12,1\n",
        "plot_graph(clean_text,start,stop,step)"
      ],
      "metadata": {
        "id": "Cu4KEUhUY2XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSA Model\n",
        "number_of_topics=7\n",
        "words=10\n",
        "document_list,titles=load_data(\"\",\"articles.txt\")\n",
        "clean_text=preprocess_data(document_list)\n",
        "model=create_gensim_lsa_model(clean_text,number_of_topics,words)\n"
      ],
      "metadata": {
        "id": "JY-8XoVQZKVO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}